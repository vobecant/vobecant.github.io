
<!DOCTYPE html>
<html lang="en">
<head>
<link rel="icon" href="data:;base64,iVBORw0KGgo=">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="stylesheet" href="style.css" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra</title>
<meta name="description" content="POP3D: Open-Vocabulary 3D Occupancy Prediction from Images. Official web for the NeurIPS'23 paper..">
<meta name="keywords" content="pop3d,clip,vobecant,avobecky,official,code" />
<meta name="author" content="Antonin Vobecky" />
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,700,1,0" />

<script type="text/javascript">(function(c,l,a,r,i,t,y){c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);})(window, document, "clarity", "script", "hnv3xao2e7");</script>
</head>
<body>
<header>
<h1>
<span class="title-main">POP-3D</span>
<span class="title-small">Open-Vocabulary 3D Occupancy Prediction from Images</span>
</h1>
</header>
<div class="authors">
<div class="author">
<span class="author-name">
<a href="https://jkulhanek.github.io/">Antonin Vobecky</a>
</span>
<span class="author-affiliation">Czech Technical University in Prague</span>
</div>
<div class="author">
<span class="author-name">
<a href="https://tsattler.github.io/">Josef Sivic</a>
</span>
<span class="author-affiliation">Czech Technical University in Prague</span>
</div>
</div>
<div class="links">
<a class="link link-paper" href="TBD">Paper</a>
<a class="link link-github" href="TBD">Code</a>

</div>
<style>
      .video.teaser-video::before {
        padding-bottom: 50%;
      }
    </style>
<div class="video teaser-video">
<video style="object-fit: cover;" alt="tetranerf intro video" width="320" height="240" loop autoplay controls muted>
<source src="./resources/intro-video.mp4" type="video/mp4">
</video>
</div>
<section class="abstract">
<h2>Abstract</h2>
<p>
We describe an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images with the objective of enabling 3D grounding, segmentation and retrieval of free-form language queries. This is a challenging problem because of the 2D-3D ambiguity and the open-vocabulary nature of the target tasks, where obtaining annotated training data in 3D is difficult. The contributions of this work are three-fold.  First, we design a new model architecture for open-vocabulary 3D semantic occupancy prediction.  The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Second, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language and (iii) LiDAR point clouds, and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations. Finally, we demonstrate quantitatively the strengths of the proposed model on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes.
<img src="resources/overview.svg" alt="TetraNeRF overview" style="width: 100%; margin: 1em auto 0.3em auto; display: block;" />
The input to Tetra-NeRF is a point cloud which is triangulated to get a set of tetrahedra used to represent the radiance field. Rays are sampled, and the field is queried. The barycentric interpolation is used to interpolate tetrahedra vertices, and the resulting features are passed to a shallow MLP to get the density and colours for volumetric rendering.
</section>
<section class="demo-section" id="demo">
<h2>Demo</h2>
<p>
We provide interactive demo showing the trained models together with
the original point clouds and tetrahedra. Please visit the <a href="./demo.html"><strong>demo page</strong></a> or click on one of the following:
</p>
<div class="demo-list">
<a class="demo-item" href="./demo.html?scene=blender-lego-sparse">
<span class="demo-item-image">
<picture>
<source srcset="./resources/images/blender-lego-sparse-100k-animated-cover.webp, ./resources/images/blender-lego-sparse-100k-animated-cover@2x.webp 2x" type="image/webp">
<source srcset="./resources/images/blender-lego-sparse-100k-animated-cover.gif, ./resources/images/blender-lego-sparse-100k-animated-cover@2x.gif 2x" type="image/gif">
<img alt="demo blender lego (sparse)" src="./resources/images/blender-lego-sparse-100k-animated-cover.gif" />
</picture>
</span>
<span class="caption">blender/lego (sparse)</span>
</a>
<a class="demo-item" href="./demo.html?scene=360-garden-sparse">
<span class="demo-item-image">
<picture>
<source srcset="./resources/images/360-garden-sparse-100k-animated-cover.webp, ./resources/images/360-garden-sparse-100k-animated-cover@2x.webp 2x" type="image/webp">
<source srcset="./resources/images/360-garden-sparse-100k-animated-cover.gif, ./resources/images/360-garden-sparse-100k-animated-cover@2x.gif 2x" type="image/gif">
<img alt="demo mipnerf360 garden (sparse)" src="./resources/images/360-garden-sparse-100k-animated-cover.gif" />
</picture>
</span>
<span class="caption">360/garden (sparse)</span>
</a>
<a class="demo-item" href="./demo.html?scene=360-bonsai-sparse">
<span class="demo-item-image">
<picture>
<source srcset="./resources/images/360-bonsai-sparse-100k-animated-cover.webp, ./resources/images/360-bonsai-sparse-100k-animated-cover@2x.webp 2x" type="image/webp">
<source srcset="./resources/images/360-bonsai-sparse-100k-animated-cover.gif, ./resources/images/360-bonsai-sparse-100k-animated-cover@2x.gif 2x" type="image/gif">
<img alt="demo mipnerf360 bonsai (sparse)" src="./resources/images/360-bonsai-sparse-100k-animated-cover.gif" />
</picture>
</span>
<span class="caption">360/bonsai (sparse)</span>
</a>
<a class="demo-item" href="./demo.html?scene=360-kitchen-dense">
<span class="demo-item-image">
<picture>
<source srcset="./resources/images/360-kitchen-dense-300k-animated-cover.webp, ./resources/images/360-kitchen-dense-300k-animated-cover@2x.webp 2x" type="image/webp">
<source srcset="./resources/images/360-kitchen-dense-300k-animated-cover.gif, ./resources/images/360-kitchen-dense-300k-animated-cover@2x.gif 2x" type="image/gif">
<img alt="demo mipnerf360 kitchen (dense)" src="./resources/images/360-kitchen-dense-300k-animated-cover.gif" />
</picture>
</span>
<span class="caption">360/kitchen (dense)</span>
</a>
</div>
</section>
<section class="citation">
<h2>Citation</h2>
<span>Please use the following citation:</span>
<pre>
@article{kulhanek2023tetranerf,
  title={{T}etra-{NeRF}: Representing Neural Radiance Fields Using Tetrahedra},
  author={Kulhanek, Jonas and Sattler, Torsten},
  journal={arXiv preprint arXiv:2304.09987},
  year={2023},
}</pre>
</section>

</body>
</html>
